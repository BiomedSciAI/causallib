{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Model\n",
    "To find the expected effect of the intervention on the population, we match each treated individual with one or more untreated individuals which are \"almost the same\" as him or her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from causallib.datasets import load_nhefs\n",
    "from causallib.estimation import IPW,Matcher\n",
    "from causallib.evaluation import PropensityEvaluator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data:\n",
    "The effect of quitting to smoke on weight loss.  \n",
    "Data example is taken from [Hernan and Robins Causal Inference Book](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>smokeintensity</th>\n",
       "      <th>smokeyrs</th>\n",
       "      <th>wt71</th>\n",
       "      <th>active_1</th>\n",
       "      <th>active_2</th>\n",
       "      <th>education_2</th>\n",
       "      <th>education_3</th>\n",
       "      <th>education_4</th>\n",
       "      <th>education_5</th>\n",
       "      <th>exercise_1</th>\n",
       "      <th>exercise_2</th>\n",
       "      <th>age^2</th>\n",
       "      <th>wt71^2</th>\n",
       "      <th>smokeintensity^2</th>\n",
       "      <th>smokeyrs^2</th>\n",
       "      <th>qsmk</th>\n",
       "      <th>wt82_71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>79.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1764</td>\n",
       "      <td>6247.3216</td>\n",
       "      <td>900</td>\n",
       "      <td>841</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.093960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>58.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1296</td>\n",
       "      <td>3437.4769</td>\n",
       "      <td>400</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "      <td>2.604970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>56.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3136</td>\n",
       "      <td>3227.3761</td>\n",
       "      <td>400</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "      <td>9.414486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>59.42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4624</td>\n",
       "      <td>3530.7364</td>\n",
       "      <td>9</td>\n",
       "      <td>2809</td>\n",
       "      <td>0</td>\n",
       "      <td>4.990117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>87.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1600</td>\n",
       "      <td>7584.6681</td>\n",
       "      <td>400</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "      <td>4.989251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  race  sex  smokeintensity  smokeyrs   wt71  active_1  active_2  \\\n",
       "0   42     1    0              30        29  79.04         0         0   \n",
       "1   36     0    0              20        24  58.63         0         0   \n",
       "2   56     1    1              20        26  56.81         0         0   \n",
       "3   68     1    0               3        53  59.42         1         0   \n",
       "4   40     0    0              20        19  87.09         1         0   \n",
       "\n",
       "   education_2  education_3  education_4  education_5  exercise_1  exercise_2  \\\n",
       "0            0            0            0            0           0           1   \n",
       "1            1            0            0            0           0           0   \n",
       "2            1            0            0            0           0           1   \n",
       "3            0            0            0            0           0           1   \n",
       "4            1            0            0            0           1           0   \n",
       "\n",
       "   age^2     wt71^2  smokeintensity^2  smokeyrs^2  qsmk    wt82_71  \n",
       "0   1764  6247.3216               900         841     0 -10.093960  \n",
       "1   1296  3437.4769               400         576     0   2.604970  \n",
       "2   3136  3227.3761               400         676     0   9.414486  \n",
       "3   4624  3530.7364                 9        2809     0   4.990117  \n",
       "4   1600  7584.6681               400         361     0   4.989251  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data = load_nhefs()\n",
    "augmented_data.X.join(augmented_data.a).join(augmented_data.y).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are looking for nearby data points to match against, the one hot encoding may not be the best choice. Augmented features are also not needed and may introduce bias. So instead of one hot encoding the categorical variables, we binarize them to \"high/low\" values and we do not augment the continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df,column_name):\n",
    "    df = df.copy()\n",
    "    m=df[column_name].median()\n",
    "    balance = lambda i : np.abs(0.5 - (df[column_name] < i).sum()/len(df))\n",
    "    mstar = min([m-1,m,m+1],key=balance)\n",
    "    df = df.assign(**{column_name:(df[column_name] < mstar).astype(int)})\n",
    "    df = df.rename(columns={column_name: column_name + f\"<{mstar}\"})\n",
    "    return df\n",
    "\n",
    "def get_matching_data():\n",
    "    data = load_nhefs(onehot=False,augment=False)\n",
    "    data.X = binarize(data.X,\"education\")\n",
    "    data.X = binarize(data.X,\"exercise\")\n",
    "    data.X = binarize(data.X,\"active\")\n",
    "    return data\n",
    "binarized_data = get_matching_data()\n",
    "X,a,y = binarized_data.X,binarized_data.a,binarized_data.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still want the propensity to use the augmented data, though, so we will pretrain a propensity vector that is indexed like the rest of the data and use it when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LogisticRegression(solver=\"liblinear\")\n",
    "learner.fit(augmented_data.X,augmented_data.a)\n",
    "propensity = learner.predict_proba(augmented_data.X)[:,0]\n",
    "propensity_df = augmented_data.X.assign(propensity=propensity)\n",
    "propensity_only_df = augmented_data.X.assign(propensity=propensity).pop(\"propensity\")\n",
    "class PropensityTransformer:\n",
    "    def __init__(self):\n",
    "        self.include_covariates=False\n",
    "    def fit(self,X,p):\n",
    "        self.X = X.assign(propensity=p)\n",
    "    def transform(self,X):\n",
    "        if self.include_covariates:\n",
    "            return self.X.loc[X.index][\"propensity\"]\n",
    "        else:\n",
    "            return self.X.loc[X.index]\n",
    "propensity_transform = PropensityTransformer()\n",
    "propensity_transform.fit(augmented_data.X,propensity)\n",
    "propensity_transform.include_covariates =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the `Matcher` class has two functional initialization parameters: `use_propensity` and `use_covariates`. We explore the differing outcomes from using the 4 possible values of these boolean parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7826753981208054"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = Matcher()\n",
    "matcher.propensity_transform = propensity_transform\n",
    "matcher.caliper = 0.001\n",
    "matcher.fit(binarized_data.X,binarized_data.a,binarized_data.y)\n",
    "Y = matcher.estimate_individual_outcome( binarized_data.X, binarized_data.a,)\n",
    "match_df_with_rep = matcher.match(X,a,y)\n",
    "(Y[1] - Y[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw = IPW(LogisticRegression(solver=\"liblinear\"))\n",
    "ipw.fit(augmented_data.X,augmented_data.a)\n",
    "Yipw = ipw.estimate_population_outcome(augmented_data.X,augmented_data.a,augmented_data.y)\n",
    "ipw_estimate = Yipw[1] - Yipw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_estimate = y[a==1].mean() - y[a==0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caliper = np.logspace(-5,0,20)\n",
    "def check_caliper(c,with_replacement=True):\n",
    "    matcher = Matcher()\n",
    "    matcher.propensity_transform = propensity_transform\n",
    "    matcher.caliper = c\n",
    "    matcher.with_replacement = with_replacement\n",
    "    matcher.fit(binarized_data.X,binarized_data.a,binarized_data.y)\n",
    "    Y = matcher.estimate_individual_outcome( binarized_data.X, binarized_data.a,)\n",
    "    Y = Y.dropna()\n",
    "    p = len(Y) / len(matcher.covariates)\n",
    "    return p, (Y[1] - Y[0]).mean()\n",
    "p_with,ATE_with = zip(*[check_caliper(c,with_replacement=True) for c in caliper])\n",
    "p_without,ATE_without = zip(*[check_caliper(c,with_replacement=False) for c in caliper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "with sb.axes_style(\"dark\") as s:\n",
    "    plt.semilogx(caliper,p_with,\"blue\")\n",
    "    plt.semilogx(caliper,p_without,\"purple\")\n",
    "    \n",
    "    plt.ylabel(\"fraction of data matched \",color=\"blue\")\n",
    "    plt.twinx()\n",
    "    plt.semilogx(caliper,ATE_with,\"green\",label=\"matching (with replacement)\")\n",
    "    plt.semilogx(caliper,ATE_without,\"orange\",label=\"matching (no replacement)\")\n",
    "    plt.ylabel(\"ATE\",color=\"green\")\n",
    "    plt.hlines(xmin=caliper.min(),xmax=caliper.max(),y = ipw_estimate, ls=\"--\",color=\"green\",label=\"ipw\")\n",
    "    plt.hlines(xmin=caliper.min(),xmax=caliper.max(),y = naive_estimate, ls=\":\",color=\"green\",label=\"naive\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlabel(\"caliper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher()\n",
    "matcher.with_replacement = True\n",
    "matcher.propensity_transform = propensity_transform\n",
    "matcher.fit(X,a,y)\n",
    "match_df = matcher.match(X,a,y)\n",
    "ATE_with_replacement = matcher.estimate_population_outcome(X,a).diff()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher()\n",
    "matcher.with_replacement = False\n",
    "matcher.propensity_transform = propensity_transform\n",
    "matcher.caliper = 0.001\n",
    "matcher.fit(X,a,y)\n",
    "match_df = matcher.match(X,a,y)\n",
    "ATE_without_replacement = matcher.estimate_population_outcome(X,a).diff()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With replacement we find:\n",
      "3.841734485172423\n",
      "Without replacement we find:\n",
      "1.650148924838708\n"
     ]
    }
   ],
   "source": [
    "print(f\"With replacement we find:\\n{ATE_with_replacement}\\nWithout replacement we find:\\n{ATE_without_replacement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the average differences between the treated and untreated are very low after matching, as expected. Now we turn to the average effect of treatment on the treated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = dict((name,matcher.get_att(binarized_data.y)) for name,matcher in matchers.items())\n",
    "pd.Series(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet,EmpiricalCovariance,ShrunkCovariance,GraphicalLassoCV\n",
    "mcd = MinCovDet()\n",
    "mcd.fit(binarized_data.X)\n",
    "ec = EmpiricalCovariance()\n",
    "ec.fit(binarized_data.X)\n",
    "sc = ShrunkCovariance()\n",
    "sc.fit(binarized_data.X)\n",
    "gl = GraphicalLassoCV()\n",
    "gl.fit(binarized_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a=plt.subplots(1,4)\n",
    "for ax,cov in zip(a.flatten(),[mcd,ec,sc,gl]):\n",
    "    #print(cov.precision_ @ cov.covariance_)\n",
    "    ax.imshow(cov.precision_ @ cov.covariance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = binarized_data.a.copy()\n",
    "match.name = \"match\"\n",
    "\n",
    "match.loc[[m.source for m in matchers[\"both\"].matches]] = [m.targets for m in matchers[\"both\"].matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "dtc.fit(binarized_data.X, matchers[\"both\"].weights > 0)\n",
    "f,ax=plt.subplots(figsize=(42,15))\n",
    "_ = plot_tree(ax=ax,decision_tree=dtc,filled=True,feature_names=matchers[\"both\"].covariates.columns,class_names=[\"unmatched\",\"matched\"],fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(binarized_data.X, matchers[\"both\"].weights > 0)\n",
    "#roc_auc_score(matchers[\"both\"].weights.values,dtc.predict(binarized_data.X))\n",
    "\n",
    "matchers[\"both\"].weights.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Intuitively the whole method only makes sense if the treated and control are actually \"near\" in some meaningful way. In particular, we want to know what we can generalize from the data we have seen to new data. Maybe dimensional reduction and visualization can help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca1,pca2 = PCA(n_components=2).fit_transform(data.X).T\n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "f,a=plt.subplots(figsize=(12,12))\n",
    "sb.scatterplot(x=\"pca1\",y=\"pca2\",hue=data.a,data=data.X.assign(pca1=pca1,pca2=pca2),ax=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tfit = TSNE().fit_transform(data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "paired =  data.a.copy(deep=True)\n",
    "for i in pairs:\n",
    "    paired.loc[i[1]]=-1\n",
    "\n",
    "def matching_as_weight(X,a,control_neighbors_of_treated):\n",
    "    weights =  data.a.copy(deep=True)\n",
    "    for i in control_neighbors_of_treated:\n",
    "        paired.loc[control.iloc[i].index] = -1\n",
    "\n",
    "f,a=plt.subplots(figsize=(10,10),subplot_kw=dict(facecolor=\".2\"))\n",
    "sb.scatterplot(x=tfit.T[0],y=tfit.T[1],hue=paired,palette=\"vlag\",alpha=0.8,ax=a)\n",
    "plt.grid(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_weight = np.ones(data.X.shape[1])\n",
    "X_t = data.X[data.a==1]\n",
    "X_c = data.X[data.a==0]\n",
    "unit_weight_t = np.random.random(X_t.shape[0])\n",
    "unit_weight_c = np.random.random(X_c.shape[0])\n",
    "def loss(w_t,X_t,w_c,X_c,feature_weights=1):\n",
    "    if feature_weights==1:\n",
    "        w_f = np.ones(X_t.shape[1])\n",
    "    ((w_t @ X_t)/w_t.sum() - (w_c @ X_c)/w_c.sum())**2 @ w_f\n",
    "    \n",
    "loss = lambda w_f: lambda w_t,w_c : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(feature_weight)(unit_weight_t,unit_weight_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_t = propensity[(data.a==1).values]\n",
    "pw_c = propensity[(data.a==0).values]\n",
    "loss(feature_weight)(pw_t,pw_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import Bunch\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO,TextIOWrapper\n",
    "\n",
    "def get_card_krueger_data(select_covariates=[\"EMPTOT\",\"WAGE_ST\",\"INCTIME\",\"BK\",\"KFC\",\"ROYS\",\"WENDYS\"],outcome=\"EMPTOT2\",dropna=True,return_column_descriptions=False):\n",
    "    \n",
    "    r=requests.get(\"https://davidcard.berkeley.edu/data_sets/njmin.zip\")\n",
    "    z = zipfile.ZipFile(BytesIO(r.content))\n",
    "    df = pd.read_csv(z.open(\"public.dat\"),engine=\"python\",sep=\"\\s+\",header=None).applymap(lambda x: pd.to_numeric(x,errors=\"coerce\"))\n",
    "    \n",
    "    ## Load column names and descriptions from `codebook`\n",
    "    codebook = [repr(line) for line in TextIOWrapper(z.open(\"codebook\"),\"cp437\")]\n",
    "    #part of the codebook is not relevant\n",
    "    codes = codebook[7:11]  + codebook[13:19] + codebook[21:38] + codebook[40:59]\n",
    "    cols = [i.strip(\"'\\\" \").split()[0] for i in codes]\n",
    "    descriptions = [\" \".join(i.strip(\"'\\\" \").split()[4:]).rstrip('\\\\n') for i in codes]\n",
    "    column_descriptions = dict(zip(cols,descriptions))\n",
    "    df.columns = cols\n",
    "\n",
    "    ## Calculate total employment following Card and Krueger's method\n",
    "    df = df.assign(EMPTOT = df.EMPFT + 0.5 * df.EMPPT + df.NMGRS)\n",
    "    df = df.assign(EMPTOT2 = df.EMPFT2 + 0.5 * df.EMPPT2 + df.NMGRS2)\n",
    "    \n",
    "    df = pd.get_dummies(df,prefix=\"name\",columns=[\"CHAIN\"]).rename(columns=dict(name_1=\"BK\",name_2=\"KFC\",name_3=\"ROYS\",name_4=\"WENDYS\"))\n",
    "    if dropna:\n",
    "        df = df.dropna(subset=select_covariates + [\"STATE\", outcome])\n",
    "    \n",
    "    ## Divide data into pre-intervention and post-intervention variables\n",
    "    x = df[[c for c in df.columns if not \"2\" in c]]\n",
    "    \n",
    "    y = df[[c for c in df.columns if \"2\" in c]]\n",
    "    y = df.pop(outcome)\n",
    "    \n",
    "    a = df.pop(\"STATE\")\n",
    "        \n",
    "    if select_covariates is not None:\n",
    "        x = x[select_covariates]\n",
    "    return Bunch(x=x,a=a,y=y,descriptors=pd.Series(column_descriptions))\n",
    "    \n",
    "\n",
    "cardkrueger = get_card_krueger_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityTransformer:\n",
    "    def __init__(self,include_covariates=False):\n",
    "        self.include_covariates=include_covariates   \n",
    "        self.learner = LogisticRegression(solver=\"liblinear\")\n",
    "    def fit(self,X,a=None,p=None):\n",
    "        if p is None:\n",
    "            if a is None:\n",
    "                raise ValueError(\"Either p or a needs to be supplied\")\n",
    "            self.learner.fit(X,a)\n",
    "            p = self.learner.predict_proba(X)[:,0]\n",
    "        self.X = X.assign(propensity=p)\n",
    "    def transform(self,X):\n",
    "        if self.include_covariates:\n",
    "            return self.X.loc[X.index][\"propensity\"]\n",
    "        else:\n",
    "            return self.X.loc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallib.datasets import load_card_krueger_data\n",
    "cardkrueger = load_card_krueger_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,a,y=cardkrueger.x,cardkrueger.a,cardkrueger.y\n",
    "match_ck=Matcher()\n",
    "propensity_transform = PropensityTransformer(False)\n",
    "propensity_transform.fit(x,a)\n",
    "match_ck.propensity_transform = propensity_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566176470588232"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_ck.fit(x,a,y)\n",
    "match_ck.with_replacement = True\n",
    "match_ck.n_neighbors = 1\n",
    "match_ck.estimate_population_outcome(x,a).diff()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.037467023643547"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipw_ck = IPW(LogisticRegression(solver=\"liblinear\"))\n",
    "ipw_ck.fit(x,a)\n",
    "ipw_ck.estimate_population_outcome(x,a,y).diff()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "x_=pd.DataFrame(np.hstack([np.arange(n),np.arange(n)]))\n",
    "a_=pd.Series(np.hstack([np.zeros(n),np.ones(n)]))\n",
    "y_=pd.Series(np.hstack([np.random.rand(n),2*np.random.rand(n)]))\n",
    "data_serial_x =  Bunch(x=x_,a=a_,y=y_)\n",
    "test_matcher = Matcher()\n",
    "test_matcher.fit(x_,a_,y_)\n",
    "test_match_df = test_matcher.match(x_,a_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def bias_correcting_estimated_outcome(self, s, t, covariates):\n",
    "        \"\"\"Bias correcting estimators for outcome following matching\n",
    "\n",
    "        Args:\n",
    "            s (integer): Treatment value to match to (usually treatment value for control)\n",
    "            t (integer): Treatment value to match from (usually treatment value for treated)\n",
    "\n",
    "        Returns:\n",
    "            tuple(pd.Series): Three different regression corrected estimators based on the\n",
    "            Difference regression, Control regression and Pooled regression methods.\n",
    "        \"\"\"\n",
    "\n",
    "        covmatch = self.get_covariates_of_matches(s, t, covariates)\n",
    "\n",
    "        # Difference Regression - regress (Y1 - Y0) ~ (X1 - X0)\n",
    "        y_d = covmatch.outcomes[s] - covmatch.outcomes[t]\n",
    "        x_d = covmatch.delta\n",
    "        fit_d = sm.OLS(y_d, sm.add_constant(x_d)).fit()\n",
    "        params_d = fit_d.params\n",
    "        yhat_d = covmatch.outcomes[t] + covmatch.delta @ params_d[1:]\n",
    "        tau_d = (covmatch.outcomes[s] - yhat_d).mean()\n",
    "\n",
    "        # Control Regression - regress Y0 ~ X0\n",
    "\n",
    "        x_c = covmatch[t].set_index(\"sample_id\")\n",
    "        y_c = covmatch.outcomes[t]\n",
    "        y_c.index = x_c.index\n",
    "\n",
    "        fit_c = sm.OLS(y_c, sm.add_constant(x_c)).fit()\n",
    "        params_c = fit_c.params\n",
    "        yhat_c = covmatch.outcomes[t] - covmatch.delta @ params_c[1:]\n",
    "        tau_c = (covmatch.outcomes[s] - yhat_c).mean()\n",
    "\n",
    "        # Pooled Regression - regress Y ~ X + A\n",
    "        x_t = covmatch[t].assign(treatment=t).set_index(\"sample_id\")\n",
    "        x_s = covmatch[s].assign(treatment=s).set_index(\"sample_id\")\n",
    "        x_p = x_s.append(x_t)\n",
    "\n",
    "        y_t = covmatch.outcomes[t]\n",
    "        y_t.index = pd.to_numeric(covmatch[t].sample_id, downcast=\"integer\")\n",
    "        y_s = covmatch.outcomes[s]\n",
    "        y_s.index = pd.to_numeric(covmatch[s].sample_id, downcast=\"integer\")\n",
    "        y_p = y_s.append(y_t)\n",
    "        fit_p = sm.OLS(y_p, sm.add_constant(x_p)).fit()\n",
    "        params_p = fit_p.params\n",
    "        yhat_p = covmatch.outcomes[t] - covmatch.delta @ params_p[1:-1]\n",
    "        tau_p = (covmatch.outcomes[s] - yhat_p).mean()\n",
    "\n",
    "        regression_outcomes = {\n",
    "            \"difference\": dict(\n",
    "                x=x_d, y=y_d, fit=fit_d, params=params_d, yhat=yhat_d, tau=tau_d\n",
    "            ),\n",
    "            \"control\": dict(\n",
    "                x=x_c, y=y_c, fit=fit_c, params=params_c, yhat=yhat_c, tau=tau_c\n",
    "            ),\n",
    "            \"pooled\": dict(\n",
    "                x=x_p, y=y_p, fit=fit_p, params=params_p, yhat=yhat_p, tau=tau_p\n",
    "            ),\n",
    "        }\n",
    "        return regression_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class OneDimensionalNearestNeighbors(object):\n",
    "    \"\"\"Simple one dimensional nearest neighbor object\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        metric (str) : Scalar distance metric. Either \"logit\" or\n",
    "           \"euclidean\". Logit is recommended for comparing \n",
    "           probabibilities, such as propensity scores.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric=\"logit\"):\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X):\n",
    "        if len(X.shape) > 1 and X.shape[1] > 1:\n",
    "            raise ValueError(\"This object only works with 1d data.\")\n",
    "        self.index = np.array(sorted([(v, idx) for idx, v in enumerate(X)]))\n",
    "        return self\n",
    "\n",
    "    def kneighbors(self, X, n_neighbors=1):\n",
    "        \"\"\"Find nearest neighbors\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Array of samples to check of shape (N,) or (N,1).\n",
    "            n_neighbors (int, optional): Number of neighbors to search for.\n",
    "                Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            (distances, indices): Two np.array objects of shape (N,n_neighbors)\n",
    "                containing the distances and indices of the closest neighbors.\n",
    "        \"\"\"\n",
    "        import bisect\n",
    "\n",
    "        def single_sample_kneighbor(x):\n",
    "            insert_idx = bisect.bisect(self.index[:, 0], x)\n",
    "            lo = max(insert_idx - n_neighbors, 0)\n",
    "            hi = min(insert_idx + n_neighbors, len(self.index))\n",
    "            distances = [(self.dist(x, val), index)\n",
    "                         for val, index in self.index[lo:hi]]\n",
    "            distances, indices = list(zip(*sorted(distances)[:n_neighbors]))\n",
    "            return distances, indices\n",
    "        distances, indices = zip(*[single_sample_kneighbor(x) for x in X])\n",
    "        #indices = np.array(indices).astype('int64')\n",
    "        return np.array(distances), np.array(indices)\n",
    "\n",
    "    def dist(self, i, j):\n",
    "        \"\"\"Distance between two scalars\n",
    "\n",
    "        Args:\n",
    "            i (float): first sample\n",
    "            j (float): second sample\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: Raised when non-supported metric is requested\n",
    "\n",
    "        Returns:\n",
    "            distance (float): distance between samples\n",
    "        \"\"\"\n",
    "        if self.metric == \"logit\":\n",
    "            return abs(np.log(i/(1-i)) - np.log(j/(1-j)))\n",
    "        elif self.metric == \"euclidean\":\n",
    "            return abs(i - j)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Metric not supported\")\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            if parameter == \"metric_params\":\n",
    "                self.__dict__.update(value)\n",
    "            else:\n",
    "                setattr(self, parameter, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
